<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Liyang Chen</title>

    <meta name="author" content="liyang Chen">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Liyang Chen (陈礼扬)
                </p>
                <p>
                I am currently a fourth-year Ph.D student at <a href="https://www.sigs.tsinghua.edu.cn/">Tsinghua University</a>, 
                supervised by <a href="https://scholar.google.com.hk/citations?user=7Xl6KdkAAAAJ">Prof. Zhiyong Wu</a>.
                </p>
                <p>
                  My research focuses on human-centric video synthesis, 2D/3D talking face generation and speech processing.
                </p>
                <p style="text-align:center">
                  <a href="mailto:cly21@mails.tsinghua.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com.hk/citations?user=jk6jWXgAAAAJ&hl">Scholar</a> &nbsp;/&nbsp;
                  <a href="image/wechat.jpg">WeChat</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="image/liyangchen.jpeg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="image/liyangchen.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


    <tr onmouseout="stabledub_stop()" onmouseover="stabledub_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='stabledub_image'><video width=160 muted autoplay loop>
          <source src="image/stabledub.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='image/stabledub.png' width="160">
        </div>
        <script type="text/javascript">
          function stabledub_start() {
            document.getElementById('stabledub_image').style.opacity = "1";
          }

          function stabledub_stop() {
            document.getElementById('stabledub_image').style.opacity = "0";
          }
          stabledub_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://stabledub.github.io">
			<span class="papertitle">StableDub : High-Quality and Generalized Visual Dubbing</span>
        </a>
        <br>
				<strong>Liyang Chen</strong>, 
				<a href="https://orcid.org/0000-0001-7088-3900">Tianze Zhou</a>,
				<a href="https://scholar.google.com.hk/citations?user=KMrFk2MAAAAJ">Xu He</a>,
        <a href="https://scholar.google.com.hk/citations?user=7Xl6KdkAAAAJ">Zhiyong Wu</a>,
        <a href="https://www.linkedin.com/in/yang-huang-92910b268/">Yang Huang</a>,
        <a href="https://scholar.google.com.hk/citations?user=T-HaQ84AAAAJ&hl">Yang Wu</a>,
        <a href="https://dblp.org/pid/70/8500.html">Zhongqian Sun</a>,
        <a href="https://dblp.org/pid/03/1094-32.html">Wei Yang</a>
				<br>
        <em>Under Review</em>, 2024
        <br>
        <a href="https://stabledub.github.io/">project page</a>
        <!-- <a href="https://arxiv.org/abs/2410.01804">arXiv</a> -->
        <p></p>
        <p>
				A hight-quality and generalized framework for audio-driven talking face generation with efficient diffusion-based network tunning.
        </p>
      </td>
    </tr>

    <tr onmouseout="magicman_stop()" onmouseover="magicman_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='magicman_image'><video width=100% height=100% muted autoplay loop>
          <source src="image/magicman.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='image/magicman.png' width="160">
        </div>
        <script type="text/javascript">
          function magicman_start() {
            document.getElementById('magicman_image').style.opacity = "1";
          }

          function magicman_stop() {
            document.getElementById('magicman_image').style.opacity = "0";
          }
          magicman_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://thuhcsi.github.io/MagicMan">
			<span class="papertitle">MagicMan: Generative Novel View Synthesis of Humans with 3D-Aware Diffusion and Iterative Refinement</span>
        </a>
        <br>
				<a href="https://scholar.google.com.hk/citations?user=KMrFk2MAAAAJ">Xu He</a>,
        <a href="https://scholar.google.com.hk/citations?user=Dt0PcAYAAAAJ&hl">Xiaoyu Li</a>, 
        <a href="https://scholar.google.com.hk/citations?user=2ztThPwAAAAJ&hl">Di Kang</a>,
        <a href="https://openreview.net/profile?id=~Jiangnan_Ye3">Jiangnan Ye</a>, 
				<a href="https://openreview.net/profile?id=~Chaopeng_Zhang1">Chaopeng Zhang</a>,
        <strong>Liyang Chen</strong>, 
        <a href="https://scholar.google.com/citations?user=qgdesEcAAAAJ&hl">Xiangjun Gao </a>,
        <a href="https://pariszhang11.github.io">Han Zhang</a>,
        <a href="https://scholar.google.com.hk/citations?user=7Xl6KdkAAAAJ">Zhiyong Wu</a>,
        <a href="https://scholar.google.cz/citations?user=i_P954wAAAAJ&hl">Haolin Zhuang</a>

        <br>
        <em>arXiv</em>, 2024
        <br>
        <a href="https://github.com/thuhcsi/MagicMan">project page</a>
        /
        <a href="https://arxiv.org/abs/2408.14211">arXiv</a>
        <p></p>
        <p>
        A human-specific multi-view diffusion model designed to generate high-quality novel view images from a single reference image.
        </p>
      </td>
    </tr>


    <tr onmouseout="adamesh_stop()" onmouseover="adamesh_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='adamesh_image'><video  width=100% muted autoplay loop>
          <source src="image/adamesh.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='image/adamesh.png' width=100%>
        </div>
        <script type="text/javascript">
          function adamesh_start() {
            document.getElementById('adamesh_image').style.opacity = "1";
          }

          function adamesh_stop() {
            document.getElementById('adamesh_image').style.opacity = "0";
          }
          adamesh_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://adamesh.github.io">
          <span class="papertitle">AdaMesh: Personalized Facial Expressions and Head Poses for Adaptive Speech-Driven 3D Facial Animation</span>
        </a>
        <br>
				<strong>Liyang Chen</strong>*,
        <a href="https://scholar.google.com.hk/citations?user=14R5bLoAAAAJ&hl">Weihong Bao*</a>,
        <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=kL2xyTYAAAAJ">Shun Lei</a>,
				<a href="https://github.com/TangYucopper">Boshi Tang</a>,
        <a href="https://scholar.google.com.hk/citations?user=7Xl6KdkAAAAJ">Zhiyong Wu</a>,
				<a href="https://scholar.google.com/citations?user=mnCHk8EAAAAJ">Shiyin Kang</a>,
				<a href="https://scholar.google.com.hk/citations?user=wTJ83eEAAAAJ">Haozhi Huang</a>,
				<a href="https://www.se.cuhk.edu.hk/people/academic-staff/prof-meng-mei-ling-helen">Helen Meng</a>
        <br>
        <em>Transaction on Multimedia</em>, 2024
        <br>
        <a href="https://adamesh.github.io">project page</a>
        /
        <a href="https://arxiv.org/abs/2310.07236">arXiv</a>
        <p></p>
        <p>
          A novel adaptive speech-driven facial animation approach, which learns the personalized talking style from a reference 
          video of about 10 seconds and generates vivid facial expressions and head poses.
        </p>
      </td>
    </tr>

    <tr onmouseout="vast_stop()" onmouseover="vast_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='vast_image'><video  width=100% muted autoplay loop>
          <source src="image/vast.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='image/vast.png' width=100%>
        </div>
        <script type="text/javascript">
          function vast_start() {
            document.getElementById('vast_image').style.opacity = "1";
          }

          function vast_stop() {
            document.getElementById('vast_image').style.opacity = "0";
          }
          vast_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2308.04830">
          <span class="papertitle">VAST: Vivify Your Talking Avatar via Zero-Shot Expressive Facial Style Transfer</span>
        </a>
        <br>
				<strong>Liyang Chen</strong>,
        <a href="https://scholar.google.com.hk/citations?user=7Xl6KdkAAAAJ">Zhiyong Wu</a>,
				<a href="https://scholar.google.com/citations?user=UlMO-z8AAAAJ">Runnan Li</a>,
        <a href="https://scholar.google.com.hk/citations?user=14R5bLoAAAAJ&hl">Weihong Bao</a>,
				<a href="https://junleen.github.io">Jun Ling</a>,
				<a href="https://scholar.google.com.hk/citations?user=tob-U1oAAAAJ">Xu Tan</a>,
				<a href="https://scholar.google.com/citations?user=689bIIwAAAAJ">Sheng Zhao</a>
        <br>
        <em>ICCV</em>, 2023
        <br>
        <a href="https://arxiv.org/abs/2308.04830">arXiv</a>
        <p></p>
        <p>
        An unsupervised variational style transfer model (VAST) to vivify the neutral photo-realistic avatars.
        It is able to flexibly capture the expressive facial style from arbitrary video prompts and transfer 
        it onto a personalized image renderer in a zero-shot manner.
        </p>
      </td>
    </tr>

    <tr onmouseout="s2a_stop()" onmouseover="s2a_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='s2a_image'><video  width=100% muted autoplay loop>
          <source src="image/s2a.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='image/s2a.png' width=100%>
        </div>
        <script type="text/javascript">
          function s2a_start() {
            document.getElementById('s2a_image').style.opacity = "1";
          }

          function s2a_stop() {
            document.getElementById('s2a_image').style.opacity = "0";
          }
          s2a_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://thuhcsi.github.io/icassp2022-Transformer-S2A">
          <span class="papertitle">Transformer-S2A: Robust and Efficient Speech-to-Animation</span>
        </a>
        <br>
        <strong>Liyang Chen</strong>,
        <a href="https://scholar.google.com.hk/citations?user=7Xl6KdkAAAAJ">Zhiyong Wu</a>,
				<a href="https://junleen.github.io">Jun Ling</a>,
				<a href="https://scholar.google.com/citations?user=UlMO-z8AAAAJ">Runnan Li</a>,
				<a href="https://scholar.google.com.hk/citations?user=tob-U1oAAAAJ">Xu Tan</a>,
				<a href="https://scholar.google.com/citations?user=689bIIwAAAAJ">Sheng Zhao</a>
        <br>
        <em>ICASSP</em>, 2022
        <br>
        <a href="https://thuhcsi.github.io/icassp2022-Transformer-S2A">project page</a>
        /
        <a href="https://www.bilibili.com/video/BV18Y4y1879s">video</a>
        /
        <a href="https://arxiv.org/abs/2111.09771">arXiv</a>
        <p></p>
        <p>
        A novel robust and efficient Speech-to-Animation (S2A) approach for synchronized facial animation generation in human-computer interaction.
        Experiments demonstrate the effectiveness of the proposed approach on both objective and subjective evaluation with 17x inference speedup 
        compared with the state-of-the-art approach
        </p>
      </td>
    </tr>

    <tr onmouseout="stableface_stop()" onmouseover="stableface_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='stableface_image'><video  width=100% height=100% muted autoplay loop>
          <source src="image/stableface.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='image/stableface.jpg' width="160">
        </div>
        <script type="text/javascript">
          function stableface_start() {
            document.getElementById('stableface_image').style.opacity = "1";
          }
  
          function stableface_stop() {
            document.getElementById('stableface_image').style.opacity = "0";
          }
          stableface_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://junleen.github.io/projects/stable-face">
          <span class="papertitle">StableFace: Analyzing and Improving Motion Stability for Talking Face Generation</span>
        </a>
        <br>
        <a href="https://junleen.github.io">Jun Ling</a>,
        <a href="https://scholar.google.com.hk/citations?user=tob-U1oAAAAJ">Xu Tan</a>,
        <strong>Liyang Chen</strong>,
        <a href="https://scholar.google.com/citations?user=UlMO-z8AAAAJ">Runnan Li</a>,
        <a href="https://ieeexplore.ieee.org/author/37086852670">Yuchao Zhang</a>,
        <a href="https://scholar.google.com/citations?user=689bIIwAAAAJ">Sheng Zhao</a>,
        <a href="https://scholar.google.com/citations?user=jKIoTVoAAAAJ">Li Song</a>
        <br>
        <em>IEEE Journal of Selected Topics in Signal Processing</em>, 2023
        <br>
        <a href="https://junleen.github.io/projects/stable-face">project page</a>
        /
        <a href="https://arxiv.org/abs/2208.13717">arXiv</a>
        <p></p>
        <p>
          we conduct systematic analyses on the motion jittering problem based on the pipeline that 
          uses 3D face representations to bridge the input audio and output video, and improve the motion stability with a series of effective designs.
        </p>
      </td>
    </tr>

    <tr onmouseout="wavsyncswap_stop()" onmouseover="wavsyncswap_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='wavsyncswap_image'>
            <img src='image/wavsyncswap.png' width=100%>
          </div>
          <img src='image/wavsyncswap.png' width=100%>
        </div>
        <script type="text/javascript">
          function wavsyncswap_start() {
            document.getElementById('wavsyncswap_image').style.opacity = "1";
          }

          function wavsyncswap_stop() {
            document.getElementById('wavsyncswapimage').style.opacity = "0";
          }
          wavsyncswap_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/10094807">
          <span class="papertitle">Wavsyncswap: End-To-End Portrait-Customized Audio-Driven Talking Face Generation</span>
        </a>
        <br>
        <a href="https://scholar.google.com.hk/citations?user=14R5bLoAAAAJ&hl">Weihong Bao</a>,
        <strong>Liyang Chen</strong>,
        <a href="https://openreview.net/profile?id=~Chaoyong_Zhou1">Chaoyong Zhou</a>,
        <a href="https://github.com/YoungSeng">Sicheng Yang</a>,
        <a href="https://scholar.google.com.hk/citations?user=7Xl6KdkAAAAJ">Zhiyong Wu</a>
        <br>
        <em>ICASSP</em>, 2023
        <br>
        <a href="https://ieeexplore.ieee.org/abstract/document/10094807">paper</a>
        <p></p>
        <p>
        Audio-driven talking face and face swapping are typically viewed as separate tasks. We propose an end-to-end model that combines these two tasks.
        </p>
      </td>
    </tr>

    <tr onmouseout="expbailando_stop()" onmouseover="expbailando_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='expbailando_image'>
            <img src='image/expbailando.png' width=100%>
          </div>
          <img src='image/expbailando.png' width=100%>
        </div>
        <script type="text/javascript">
          function expbailando_start() {
            document.getElementById('expbailando_image').style.opacity = "1";
          }

          function expbailando_stop() {
            document.getElementById('expbailandoimage').style.opacity = "0";
          }
          expbailando_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2403.05834">
          <span class="papertitle">Enhancing Expressiveness in Dance Generation via Integrating Frequency and Music Style Information</span>
        </a>
        <br>
        <a href="https://scholar.google.com/citations?user=WGcnqWYAAAAJ">Qiaochu Huang</a>
        <a href="https://scholar.google.com.hk/citations?user=KMrFk2MAAAAJ">Xu He</a>,
        <a href="https://github.com/TangYucopper">Boshi Tang</a>,
        <a href="https://scholar.google.cz/citations?user=i_P954wAAAAJ&hl">Haolin Zhuang</a>
        <strong>Liyang Chen</strong>,
        <a href="https://ieeexplore.ieee.org/author/909470503451788">Shuochen Gao</a>,
        <a href="https://scholar.google.com.hk/citations?user=7Xl6KdkAAAAJ">Zhiyong Wu</a>,
        <a href="https://scholar.google.com.hk/citations?user=wTJ83eEAAAAJ">Haozhi Huang</a>,
        <a href="https://www.se.cuhk.edu.hk/people/academic-staff/prof-meng-mei-ling-helen">Helen Meng</a>
        <br>
        <em>ICASSP</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2403.05834">arXiv</a>
        <p></p>
        <p>A novel dance generation method designed to generate expressive dances, concurrently taking genre matching, beat alignment and dance dynamics into account.</p>
      </td>
    </tr>

    <tr onmouseout="hiespeech_stop()" onmouseover="hiespeech_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='hiespeech_image'>
            <img src='image/hiespeech.png' width=100%>
          </div>
          <img src='image/hiespeech.png' width=160>
        </div>
        <script type="text/javascript">
          function hiespeech_start() {
            document.getElementById('hiespeech_image').style.opacity = "1";
          }

          function hiespeech_stop() {
            document.getElementById('hiespeech_image').style.opacity = "0";
          }
          hiespeech_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2203.12201">
      <span class="papertitle">Towards Expressive Speaking Style Modelling with Hierarchical Context Information for Mandarin Speech Synthesis</span>
        </a>
        <br>
        <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=kL2xyTYAAAAJ">Shun Lei</a>,
        <a href="https://scholar.google.com/citations?user=ZI_Cm1cAAAAJ">Yixuan Zhou</a>,
        <strong>Liyang Chen</strong>,
        <a href="https://scholar.google.com.hk/citations?user=7Xl6KdkAAAAJ">Zhiyong Wu</a>,
        <a href="https://scholar.google.com/citations?user=mnCHk8EAAAAJ">Shiyin Kang</a>,
        <a href="https://www.se.cuhk.edu.hk/people/academic-staff/prof-meng-mei-ling-helen">Helen Meng</a>
        <br>
        <em>ICASSP</em>, 2022
        <br>
        <a href="https://arxiv.org/abs/2203.12201">arXiv</a>
        <p></p>
        <p>
          Previous works on expressive speech synthesis mainly focus on current sentence. 
          The context in adjacent sentences is neglected, resulting in inflexible speaking style for the same text, which lacks speech variations.
        </p>
      </td>
    </tr>
	

    <tr onmouseout="genea2022_stop()" onmouseover="genea2022_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='genea2022_image'>
					  <img src='image/genea2022.png' width=100%>
					</div>
          <img src='image/genea2022.png' width=100%>
        </div>
        <script type="text/javascript">
          function genea2022_start() {
            document.getElementById('genea2022_image').style.opacity = "1";
          }

          function genea2022_stop() {
            document.getElementById('genea2022_image').style.opacity = "0";
          }
          genea2022_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2208.12133">
          <span class="papertitle">The ReprGesture entry to the GENEA Challenge 2022</span>
        </a>
        <br>
          <a href="https://github.com/YoungSeng">Sicheng Yang</a>,
          <a href="https://scholar.google.com.hk/citations?user=7Xl6KdkAAAAJ">Zhiyong Wu</a>,
          <a href="https://scholar.google.com/citations?hl=en&user=KqU6kVcAAAAJ">Minglei Li</a>,
          <a href="https://scholar.google.com/citations?hl=en&user=nLgORGMAAAAJ">Mengchen Zhao</a>,
          <a href="https://github.com/lin9x">Jiuxin Lin</a>,
          <strong>Liyang Chen</strong>,
          <a href="https://scholar.google.com.hk/citations?user=14R5bLoAAAAJ&hl">Weihong Bao</a>
        <br>
        <em>ICMI</em>, 2022
        <br>
        <a href="https://github.com/YoungSeng/ReprGesture">project page</a>
        /
        <a href="https://arxiv.org/abs/2208.12133">arXiv</a>
        <p></p>
        <p>
        We explore an automatic gesture generation system based on multimodal representation learning
        </p>
      </td>
    </tr>


    <tr onmouseout="gtn_stop()" onmouseover="gtn_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
        <div class="two" id='gtn_image'><video  width=100% height=100% muted autoplay loop>
        <source src="image/gtn.mp4" type="video/mp4">
        Your browser does not support the video tag.
        </video></div>
          <img src='image/gtn.png' width="160">
        </div>
        <script type="text/javascript">
          function gtn_start() {
            document.getElementById('gtn_image').style.opacity = "1";
          }

          function gtn_stop() {
            document.getElementById('gtn_image').style.opacity = "0";
          }
          gtn_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://im1eon.github.io/ICASSP23-GTNB-DG">
          <span class="papertitle">GTN-Bailando: Genre Consistent long-Term 3D Dance Generation Based on Pre-Trained Genre Token Network</span>
        </a>
        <br>
          <a href="https://scholar.google.cz/citations?user=i_P954wAAAAJ&hl">Haolin Zhuang</a>
          <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=kL2xyTYAAAAJ">Shun Lei</a>,
          <a href="https://dblp.org/search/pid/api?q=author:Long_Xiao">Long Xiao</a>,
          <a href="https://ieeexplore.ieee.org/author/37090057639">Weiqin Li</a>,
          <strong>Liyang Chen</strong>,
          <a href="https://github.com/YoungSeng">Sicheng Yang</a>,
          <a href="https://scholar.google.com.hk/citations?user=7Xl6KdkAAAAJ">Zhiyong Wu</a>,
          <a href="https://scholar.google.com/citations?user=mnCHk8EAAAAJ">Shiyin Kang</a>,
          <a href="https://www.se.cuhk.edu.hk/people/academic-staff/prof-meng-mei-ling-helen">Helen Meng</a>
        <br>
      <em>ICASSP<em>, 2023
        <br>
        <a href="https://im1eon.github.io/ICASSP23-GTNB-DG">project page</a>
        /
        <a href="https://arxiv.org/abs/2304.12704">arXiv</a>
        <p></p>
        <p>
        A dance generation framework to enhance the genre consistency of long-term dance generation.
        </p>
      </td>
    </tr>          

    <tr onmouseout="mssstyle_stop()" onmouseover="mssstyle_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='mssstyle_image'>
            <img src='image/mssstyle.png' width=100%>
          </div>
          <img src='image/mssstyle.png' width=100%>
        </div>
        <script type="text/javascript">
          function mssstyle_start() {
            document.getElementById('mssstyle_image').style.opacity = "1";
          }

          function mssstyle_stop() {
            document.getElementById('mssstyleimage').style.opacity = "0";
          }
          mssstyle_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2307.16012">
          <span class="papertitle">MSStyleTTS: Multi-Scale Style Modeling With Hierarchical Context Information for Expressive Speech Synthesis</span>
        </a>
        <br>
        <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=kL2xyTYAAAAJ">Shun Lei</a>,
      <a href="https://scholar.google.com/citations?user=ZI_Cm1cAAAAJ">Yixuan Zhou</a>,
      <strong>Liyang Chen</strong>,
      <a href="https://scholar.google.com.hk/citations?user=7Xl6KdkAAAAJ">Zhiyong Wu</a>,
      <a href="https://scholar.google.com.hk/citations?user=y8O77DYAAAAJ&hl=en">Xixin Wu</a>,
      <a href="https://scholar.google.com/citations?user=mnCHk8EAAAAJ">Shiyin Kang</a>,
      <a href="https://www.se.cuhk.edu.hk/people/academic-staff/prof-meng-mei-ling-helen">Helen Meng</a>
        <br>
        <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 2023
        <br>
        <a href="https://arxiv.org/abs/2307.16012">arXiv</a>
        <p></p>
        <p>
        A style modeling method for expressive speech synthesis, to capture and predict styles at different levels from a wider range of context rather than a sentence.
        </p>
      </td>
    </tr>

    <tr onmouseout="speakergan_stop()" onmouseover="speakergan_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='speakergan_image'>
            <img src='image/speakergan.png' width=100%>
          </div>
          <img src='image/speakergan.png' width=160>
        </div>
        <script type="text/javascript">
          function speakergan_start() {
            document.getElementById('speakergan_image').style.opacity = "1";
          }
  
          function speakergan_stop() {
            document.getElementById('speakergan_image').style.opacity = "0";
          }
          speakergan_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://www.sciencedirect.com/science/article/pii/S0925231220313163">
          <span class="papertitle">SpeakerGAN: Speaker Identification with Conditional Generative Adversarial Network</span>
        </a>
        <br>
          <strong>Liyang Chen</strong>,
          <a href="https://ieeexplore.ieee.org/author/37087473495">Yifeng Liu</a>,
          <a href="https://scholar.google.com/citations?user=7lE_9vwAAAAJ">Wendong Xiao</a>,
          <a href="https://openreview.net/profile?id=~Yingxue_Wang2">Yingxue Wang</a>,
          <a href="https://scholar.google.com.hk/citations?user=irReiV0AAAAJ">Haiyong Xie</a>
          <br>
        <em>Neurocomputing</em>, 2020
        <br>
        <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231220313163">paper</a>
        <p></p>
        <p>
        SpeakerGAN allows the adversarial networks for distinguishing real/fake speech samples and predicting class labels simultaneously.
        </p>
      </td>
    </tr>

    </tbody></table>
          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Work Experiences</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="image/moonshot_logo.png" width=160></td>
              
              <td style="padding:10px;width:45%;text-align:left">
                <a href="https://www.moonshot.cn">MoonShot AI, Research Intern</a>
                <br>
                <b>Supervisor</b>: <a href="https://scholar.google.com.hk/citations?user=tob-U1oAAAAJ">Xu Tan</a>
              </td>
              <td style="padding:0px;width:30%;text-align:left; white-space: nowrap;">
                <strong>Dec. 2024 - Present</strong>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="image/tencent_logo.png" width=160></td>
              
              <td style="padding:10px;width:45%;text-align:left">
                <a href="https://ailab.tencent.com/ailab/en/index">Tencent AIPD/AILab, Research Intern</a>
                <br>
                <b>Supervisor</b>: <a href="https://scholar.google.com.hk/citations?hl=en&user=T-HaQ84AAAAJ">Yang Wu,</a> <a href="https://scholar.google.com/citations?hl=en&user=xUMuDgwAAAAJ">Jun Zhang</a>
              </td>
              <td style="padding:0px;width:30%;text-align:left; white-space: nowrap;">
                <strong>Feb. 2024 - Dec. 2024</strong>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="image/xverse_logo.png" width=160></td>
              <td style="padding:10px;width:45%;text-align:left">
                <a href="https://www.xverse.cn">XVerse Technology, Research Intern</a>
                <br>
                <b>Supervisor</b>: <a href="https://scholar.google.com/citations?user=mnCHk8EAAAAJ">Shiyin Kang,</a> <a href="https://scholar.google.com.hk/citations?user=wTJ83eEAAAAJ">Haozhi Huang</a>
              </td>
              <td style="padding:0px;width:30%;text-align:left; white-space: nowrap;">
                <strong>Oct. 2022 - Nov. 2023</strong>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="image/microsoft_logo.svg" width=160></td>
              <td style="padding:10px;width:45%;text-align:left">
                <a href="https://www.microsoft.com/en-us/research/">Microsoft Asia, Research Intern</a>
                <br>
                <b>Supervisor</b>: <a href="https://scholar.google.com/citations?user=UlMO-z8AAAAJ">Runnan Li,</a> <a href="https://scholar.google.com.hk/citations?user=tob-U1oAAAAJ">Xu Tan
              </td>
              <td style="padding:0px;width:30%;text-align:left; white-space: nowrap;">
                <strong>Apr. 2021 - Aug. 2022</strong>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="image/bytedance_logo.svg" width=160></td>
              <td style="padding:10px;width:45%;text-align:left">
                <a href="https://www.bytedance.com/en">ByteDance AILab Speech, Research Intern</a>
                <br>
                <b>Supervisor</b>: <a href="https://ieeexplore.ieee.org/author/37088481100">Shichao Liu
              </td>
              <td style="padding:0px;width:30%;text-align:left; white-space: nowrap;">
                <strong>Jun. 2020 - Mar. 2021</strong>
              </td>
            </tr>

            
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  This page is borrowd from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
